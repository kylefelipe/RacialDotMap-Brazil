{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando os dados de raça e cor\n",
    "\n",
    "Os dados de raça e cor são gerados utilizando o método criado pelo [http://patadata.org](http://patadata.org).\n",
    "\n",
    "Primeio é necessário fazer o download dos seguintes dados:\n",
    "\n",
    "- Polígonos de setores censitários, usando o arquivo [download_setores.ipynb](./download_setores.ipynb), especificando na variável correta o ano de `2022`.\n",
    "- Dados do censo de 2022, usando o arquivo [download_censo.ipynb](./download_setores.ipynb), especificando o ano de `2022` na variável correta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from os import path, makedirs, getenv\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informações necessárias para a execução do código "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paths\n",
    "# pasta onde estão os dados de cor e raça\n",
    "base_path = \"./COR_RACA/2022\"\n",
    "csv_folder = path.join(base_path, \"CSVS\")  # pasta onde estão os arquivos csv (censo)\n",
    "rand_folder = path.join(\n",
    "    base_path, \"pontos_aleatorios\"\n",
    ")  # pasta onde serão salvos os arquivos de pontos aleatórios\n",
    "makedirs(rand_folder, exist_ok=True)\n",
    "\n",
    "# Pasta dos setores censitários\n",
    "setor_sensitario_folder = path.join(\".\", \"SETORES_CENSITARIOS\", \"2022\", \"gpkg\")\n",
    "\n",
    "agregado_file = next(\n",
    "    iter(glob(path.join(csv_folder, \"*.csv\")))\n",
    ")  # arquivo de dados agregados do censo\n",
    "states = [\n",
    "    \"AC\",\n",
    "    \"AL\",\n",
    "    \"AM\",\n",
    "    # \"AP\",\n",
    "    # \"BA\",\n",
    "    \"CE\",\n",
    "    # \"DF\",\n",
    "    # \"ES\",\n",
    "    # \"GO\",\n",
    "    # \"MA\",\n",
    "    \"MG\",\n",
    "    \"MS\",\n",
    "    \"MT\",\n",
    "    \"PA\",\n",
    "    \"PB\",\n",
    "    # \"PE\", #! Parrou na organização do arquivo\n",
    "    \"PI\",\n",
    "    \"PR\",\n",
    "    \"RJ\",\n",
    "    \"RN\",\n",
    "    \"RO\",\n",
    "    # \"RR\",\n",
    "    \"RS\",\n",
    "    \"SC\",\n",
    "    # \"SE\",\n",
    "    # \"SP\", #! Parrou na organização do arquivo\n",
    "    # \"TO\",\n",
    "    # \"SP\",\n",
    "]  # estados que serão processados\n",
    "\n",
    "states = [\"TO\", \"SE\", \"RO\", \"PA\", \"PR\", \"MD\"]\n",
    "\n",
    "# Campos de interesse do arquivo de censo, com seus respectivos nomes\n",
    "pessoa_fields = {\n",
    "    \"V01317\": \"pessoas_brancas\",\n",
    "    \"V01318\": \"pessoas_pretas\",\n",
    "    \"V01319\": \"pessoas_amarelas\",\n",
    "    \"V01320\": \"pessoas_pardas\",\n",
    "    \"V01321\": \"pessoas_indigenas\",\n",
    "    \"CD_SETOR\": \"CD_SETOR\",\n",
    "}\n",
    "\n",
    "# Tipo dos campos do arquivo de censo\n",
    "pessoa_fields_dypes = {\n",
    "    \"V01317\": \"Int64\",\n",
    "    \"V01318\": \"Int64\",\n",
    "    \"V01319\": \"Int64\",\n",
    "    \"V01320\": \"Int64\",\n",
    "    \"V01321\": \"Int64\",\n",
    "    \"CD_SETOR\": \"object\",\n",
    "}\n",
    "\n",
    "# Campos de interesse do arquivo de setores censitários com seus respectivos nomes\n",
    "setor_fiels = {\"v0001\": \"total_pessoas\"}\n",
    "\n",
    "# Campos do setor censitário que serão utilizados no processo\n",
    "setor_keep_fields = [\n",
    "    \"CD_SETOR\",\n",
    "    \"SITUACAO\",\n",
    "    \"CD_SIT\",\n",
    "    \"CD_TIPO\",\n",
    "    \"AREA_KM2\",\n",
    "    \"CD_REGIAO\",\n",
    "    \"NM_REGIAO\",\n",
    "    \"CD_UF\",\n",
    "    \"NM_UF\",\n",
    "    \"CD_MUN\",\n",
    "    \"NM_MUN\",\n",
    "    \"CD_DIST\",\n",
    "    \"NM_DIST\",\n",
    "    \"CD_SUBDIST\",\n",
    "    \"NM_SUBDIST\",\n",
    "    \"CD_BAIRRO\",\n",
    "    \"NM_BAIRRO\",\n",
    "    \"CD_NU\",\n",
    "    \"NM_NU\",\n",
    "    \"CD_FCU\",\n",
    "    \"NM_FCU\",\n",
    "    \"CD_AGLOM\",\n",
    "    \"NM_AGLOM\",\n",
    "    \"CD_RGINT\",\n",
    "    \"NM_RGINT\",\n",
    "    \"CD_RGI\",\n",
    "    \"NM_RGI\",\n",
    "    \"CD_CONCURB\",\n",
    "    \"NM_CONCURB\",\n",
    "    \"v0001\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "# Campos de raça e cor e o label que será usado nos pontos\n",
    "race_fields = {\n",
    "    \"pessoas_brancas\": \"Brancos\",\n",
    "    \"pessoas_pretas\": \"Pretos\",\n",
    "    \"pessoas_amarelas\": \"Amarelas\",\n",
    "    \"pessoas_pardas\": \"Pardos\",\n",
    "    \"pessoas_indigenas\": \"Indígenas\",\n",
    "}\n",
    "\n",
    "# Campos que serão mantidos no arquivo final de pontos\n",
    "points_cols = [\n",
    "    \"CD_SETOR\",\n",
    "    \"SITUACAO\",\n",
    "    \"CD_SIT\",\n",
    "    \"CD_TIPO\",\n",
    "    \"AREA_KM2\",\n",
    "    \"CD_REGIAO\",\n",
    "    \"NM_REGIAO\",\n",
    "    \"CD_UF\",\n",
    "    \"NM_UF\",\n",
    "    \"CD_MUN\",\n",
    "    \"NM_MUN\",\n",
    "    \"CD_DIST\",\n",
    "    \"NM_DIST\",\n",
    "    \"CD_SUBDIST\",\n",
    "    \"NM_SUBDIST\",\n",
    "    \"CD_BAIRRO\",\n",
    "    \"NM_BAIRRO\",\n",
    "    \"CD_NU\",\n",
    "    \"NM_NU\",\n",
    "    \"CD_FCU\",\n",
    "    \"NM_FCU\",\n",
    "    \"CD_AGLOM\",\n",
    "    \"NM_AGLOM\",\n",
    "    \"CD_RGINT\",\n",
    "    \"NM_RGINT\",\n",
    "    \"CD_RGI\",\n",
    "    \"NM_RGI\",\n",
    "    \"CD_CONCURB\",\n",
    "    \"NM_CONCURB\",\n",
    "    \"total_pessoas\",\n",
    "    *race_fields.keys(),\n",
    "    \"raca_cor\",\n",
    "    \"tile_x\",\n",
    "    \"tile_y\",\n",
    "    \"quadkey\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "raca_cor_poligono = path.join(\".\", \"COR_RACA\", \"2022\", \"poligonos\", \"raca_cor_pol.gpkg\")\n",
    "makedirs(path.dirname(raca_cor_poligono), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pessoa_df = pd.read_csv(\n",
    "    agregado_file,\n",
    "    sep=\";\",\n",
    "    encoding=\"latin1\",\n",
    "    usecols=[*pessoa_fields.keys()],  # * abrimos apenas as colunas que precisamos\n",
    "    na_values=[\"X\"],  # * aqui estamos transformando em x em NaN/None\n",
    "    dtype=pessoa_fields_dypes,  # * aqui estamos informando os tipos corretos dos campos\n",
    ")\n",
    "\n",
    "pessoa_df.rename(columns=pessoa_fields, inplace=True) # * renomeando as colunas\n",
    "\n",
    "pessoa_cols = pessoa_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:   4%|▎         | 1/27 [00:00<00:14,  1.84arquivo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ap processado - 1499 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:   7%|▋         | 2/27 [00:04<00:56,  2.27s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado pe processado - 19700 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  11%|█         | 3/27 [00:21<03:37,  9.04s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado sp processado - 103620 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  15%|█▍        | 4/27 [00:24<02:35,  6.74s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ma processado - 16368 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  19%|█▊        | 5/27 [00:24<01:39,  4.50s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado rr processado - 1986 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  22%|██▏       | 6/27 [00:30<01:44,  5.00s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ba processado - 31070 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  26%|██▌       | 7/27 [00:32<01:19,  3.97s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado to processado - 4119 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  30%|██▉       | 8/27 [00:33<00:58,  3.08s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado se processado - 5347 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  33%|███▎      | 9/27 [00:37<01:00,  3.34s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado go processado - 12861 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  37%|███▋      | 10/27 [00:39<00:49,  2.89s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado es processado - 8788 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  41%|████      | 11/27 [00:40<00:36,  2.31s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado df processado - 5418 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  44%|████▍     | 12/27 [00:50<01:09,  4.65s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado mg processado - 51392 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  48%|████▊     | 13/27 [00:51<00:48,  3.47s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ro processado - 3456 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  52%|█████▏    | 14/27 [00:54<00:44,  3.41s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado pa processado - 18635 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  56%|█████▌    | 15/27 [00:55<00:32,  2.73s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado rn processado - 6096 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  59%|█████▉    | 16/27 [00:57<00:26,  2.41s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado pi processado - 7340 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  63%|██████▎   | 17/27 [00:58<00:18,  1.89s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ac processado - 2215 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  67%|██████▋   | 18/27 [01:02<00:23,  2.57s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado pr processado - 23899 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  70%|███████   | 19/27 [01:03<00:17,  2.23s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ms processado - 6169 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  74%|███████▍  | 20/27 [01:07<00:18,  2.58s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado ce processado - 20982 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  78%|███████▊  | 21/27 [01:08<00:13,  2.25s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado al processado - 6360 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  81%|████████▏ | 22/27 [01:10<00:10,  2.12s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado pb processado - 9644 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  85%|████████▌ | 23/27 [01:14<00:11,  2.83s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado sc processado - 16831 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  89%|████████▉ | 24/27 [01:17<00:07,  2.65s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado mt processado - 9385 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  93%|█████████▎| 25/27 [01:21<00:06,  3.18s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado rs processado - 25575 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos:  96%|█████████▋| 26/27 [01:28<00:04,  4.29s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado rj processado - 42270 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando arquivos: 100%|██████████| 27/27 [01:31<00:00,  3.40s/arquivo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo do estado am processado - 11755 registros\n",
      "Total de registros processados: 472780\n",
      "Fim do processamento\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lista os arquivos dos estados dentro da pasta de setores sensitários usando glob\n",
    "setores_sensitarios_files = glob(path.join(setor_sensitario_folder, \"*.gpkg\"))\n",
    "# setores_sensitarios_files = filtra_arquvios_sigla_estados(setores_sensitarios_files, states)\n",
    "\n",
    "\n",
    "bar_files = tqdm(\n",
    "    setores_sensitarios_files,\n",
    "    desc=\"Processando arquivos\",\n",
    "    unit=\"arquivo\",\n",
    "    total=len(setores_sensitarios_files),\n",
    "    position=0,\n",
    "    leave=True,\n",
    ")\n",
    "files_bar_log = tqdm(total=0, position=1, bar_format=\"{desc}\")\n",
    "total_len = 0\n",
    "for setor_censitario_path in bar_files:\n",
    "    sigla_uf = path.basename(setor_censitario_path).split(\"_\")[0].lower()\n",
    "    # if sigla_uf.upper() not in states:\n",
    "    #     continue\n",
    "    files_bar_log.set_description_str(f\"Processando {sigla_uf}\")\n",
    "    setor_censitario_gdf = gpd.read_file(setor_censitario_path)\n",
    "\n",
    "    # Remove colunas que não são necessárias\n",
    "    setor_censitario_gdf.drop(\n",
    "        columns=[c for c in setor_censitario_gdf.columns if c not in setor_keep_fields],\n",
    "        inplace=True,\n",
    "    )\n",
    "    gdf_len = setor_censitario_gdf.shape[0]\n",
    "    total_len += gdf_len\n",
    "    # Renomeia as colunas\n",
    "    setor_censitario_gdf.rename(columns=setor_fiels, inplace=True)\n",
    "    # Fazendo a união das tabelas\n",
    "    joined_gdf = setor_censitario_gdf.merge(pessoa_df, on=\"CD_SETOR\", how=\"left\")\n",
    "    # limpa a memória do setor censitário\n",
    "    del setor_censitario_gdf\n",
    "    # limpa a memória do setor censitário\n",
    "\n",
    "    cols = joined_gdf.columns\n",
    "    cols = [col.lower() for col in cols]\n",
    "    joined_gdf.columns = cols\n",
    "    mode = \"a\" if path.exists(raca_cor_poligono) else \"w\"\n",
    "    joined_gdf.to_file(raca_cor_poligono, driver=\"GPKG\", mode=mode)\n",
    "    print(f\"Arquivo do estado {sigla_uf} processado - {gdf_len} registros\")\n",
    "\n",
    "    del joined_gdf\n",
    "\n",
    "print(f\"Total de registros processados: {total_len}\")\n",
    "print(\"Fim do processamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordenando o arquivo\n",
      "Arquivo ordenado\n"
     ]
    }
   ],
   "source": [
    "# Ordenando o arquivo\n",
    "print(\"Ordenando o arquivo\")\n",
    "raca_cor_gdf = gpd.read_file(raca_cor_poligono)\n",
    "cols_to_order = [\"cd_uf\", \"cd_mun\", \"cd_setor\"]\n",
    "raca_cor_gdf.sort_values(by=cols_to_order, inplace=True)\n",
    "raca_cor_gdf.to_file(raca_cor_poligono, driver=\"GPKG\", mode=\"w\")\n",
    "print(\"Arquivo ordenado\")\n",
    "print(\"tamanho: \", raca_cor_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando arquivo no Postgis postgresql://postgres:ZnPdqvZhGycO5xlcJAYSUqtOgl6PnoAulmBVWc9xBTriZ3oEFlIuUlsYQRFr8KA9@localhost:5555/geoportal_backend\n"
     ]
    }
   ],
   "source": [
    "# Salvando arquivo no Postgis\n",
    "DB_URI = getenv(\"DB_URI\")\n",
    "\n",
    "print(f\"Salvando arquivo no Postgis {DB_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando chunk 0 - slice(0, 10000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 1 - slice(10000, 20000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 2 - slice(20000, 30000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 3 - slice(30000, 40000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 4 - slice(40000, 50000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 5 - slice(50000, 60000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 6 - slice(60000, 70000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 7 - slice(70000, 80000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 8 - slice(80000, 90000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 9 - slice(90000, 100000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 10 - slice(100000, 110000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 11 - slice(110000, 120000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 12 - slice(120000, 130000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 13 - slice(130000, 140000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 14 - slice(140000, 150000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 15 - slice(150000, 160000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 16 - slice(160000, 170000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 17 - slice(170000, 180000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 18 - slice(180000, 190000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 19 - slice(190000, 200000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 20 - slice(200000, 210000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 21 - slice(210000, 220000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 22 - slice(220000, 230000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 23 - slice(230000, 240000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 24 - slice(240000, 250000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 25 - slice(250000, 260000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 26 - slice(260000, 270000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 27 - slice(270000, 280000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 28 - slice(280000, 290000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 29 - slice(290000, 300000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 30 - slice(300000, 310000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 31 - slice(310000, 320000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 32 - slice(320000, 330000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 33 - slice(330000, 340000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 34 - slice(340000, 350000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 35 - slice(350000, 360000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 36 - slice(360000, 370000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 37 - slice(370000, 380000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 38 - slice(380000, 390000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 39 - slice(390000, 400000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 40 - slice(400000, 410000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 41 - slice(410000, 420000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 42 - slice(420000, 430000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 43 - slice(430000, 440000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 44 - slice(440000, 450000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 45 - slice(450000, 460000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 46 - slice(460000, 470000, None)\n",
      "Tamanho do chunk 10000\n",
      "Salvando chunk 47 - slice(470000, 480000, None)\n",
      "Tamanho do chunk 2780\n",
      "Salvando chunk 48 - slice(480000, 490000, None)\n",
      "Tamanho do chunk 0\n",
      "Arquivo vazio\n",
      "Arquivo salvo no Postgis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "engine = create_engine(DB_URI)\n",
    "chunk_size = 10000\n",
    "chunk = 0\n",
    "while True:\n",
    "    if_exists = \"replace\" if chunk == 0 else \"append\"\n",
    "    try:\n",
    "        # using slice to get a chunk of the dataframe\n",
    "        rows = slice(chunk_size * chunk, chunk_size * (chunk + 1))\n",
    "        print(f\"Salvando chunk {chunk} - {rows}\")\n",
    "        raca_cor_gdf = gpd.read_file(raca_cor_poligono, rows=rows)\n",
    "        print(f\"Tamanho do chunk {raca_cor_gdf.shape[0]}\")\n",
    "        raca_cor_gdf.to_postgis(\"raca_cor_pol\", engine, if_exists=if_exists, index=False)\n",
    "        if raca_cor_gdf.shape[0] == 0:\n",
    "            print(\"Arquivo vazio\")\n",
    "            break\n",
    "        chunk += 1\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao salvar no Postgis\")\n",
    "        print(e)\n",
    "        break\n",
    "print(\"Arquivo salvo no Postgis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
